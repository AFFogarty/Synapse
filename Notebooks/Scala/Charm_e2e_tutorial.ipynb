{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Charm\n",
        "Charm introduces the ability for Spark users to leverage indexes on their data for potential query and workload acceleration.\n",
        "\n",
        "This tutorial walks through an example scenario to highlight the basics of Charm and shows how it can be used."
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "As first step, we need to start a new Spark session. We also make some configuration change for the purpose of this tutorial: By default, Spark uses broadcast join to optimize join queries when the data size for one side of join is small (which is the case for the sample data we use in this tutorial). Therefore, we disable broadcast joins so that later when we run join queries, Spark uses SortMerge join. This is mainly to show how Charm indexes would be used at scale for a join query.\n",
        "\n",
        "Output of running below cell shows a reference to the successfully created Spark session and prints out '-1' as the value for the modified join config which indicates that broadcast join is successfully disabled.       \n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "res3: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@39fe1ddb\n-1"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Start your Spark session\n",
        "spark\n",
        "\n",
        "// Disable BroadcastHashJoin, so Spark will use standard SortMergeJoin. Currently charm indexes utilize SortMergeJoin to speed up query.\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "\n",
        "// Verify that BroadcastHashJoin is set correctly \n",
        "println(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "As the first step, we need to create sample data records and save them as parquet data files. \n",
        "We will use these records to create several Charm indexes and show how one can make Spark use them when running queries. Our example records correspond to two datasets: *department* and *employee*. You should configure \"empLocation\" and \"deptLocation\" paths so that on the storage account they point to your desired location to save generated data files. \n",
        "\n",
        "Output of running below cell shows contents of our datasets as lists of triplets followed by references to dataFrames created to save the content of each dataset in our preferred location.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "import org.apache.spark.sql.DataFrame\ndepartments: Seq[(Int, String, String)] = List((10,Accounting,New York), (20,Research,Dallas), (30,Sales,Chicago), (40,Operations,Boston))\nemployees: Seq[(Int, String, Int)] = List((7369,SMITH,20), (7499,ALLEN,30), (7521,WARD,30), (7566,JONES,20), (7698,BLAKE,30), (7782,CLARK,10), (7788,SCOTT,20), (7839,KING,10), (7844,TURNER,30), (7876,ADAMS,20), (7900,JAMES,30), (7934,MILLER,10), (7902,FORD,20), (7654,MARTIN,30))\nimport spark.implicits._\nempData: org.apache.spark.sql.DataFrame = [empId: int, empName: string ... 1 more field]\ndeptData: org.apache.spark.sql.DataFrame = [deptId: int, deptName: string ... 1 more field]\nempLocation: String = /apdave/employees.parquet\ndeptLocation: String = /apdave/departments.parquet"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import org.apache.spark.sql.DataFrame\n",
        "\n",
        "// Sample department records\n",
        "val departments = Seq(\n",
        "      (10, \"Accounting\", \"New York\"),\n",
        "      (20, \"Research\", \"Dallas\"),\n",
        "      (30, \"Sales\", \"Chicago\"),\n",
        "      (40, \"Operations\", \"Boston\"))\n",
        "\n",
        "// Sample employee records\n",
        "val employees = Seq(\n",
        "      (7369, \"SMITH\", 20),\n",
        "      (7499, \"ALLEN\", 30),\n",
        "      (7521, \"WARD\", 30),\n",
        "      (7566, \"JONES\", 20),\n",
        "      (7698, \"BLAKE\", 30),\n",
        "      (7782, \"CLARK\", 10),\n",
        "      (7788, \"SCOTT\", 20),\n",
        "      (7839, \"KING\", 10),\n",
        "      (7844, \"TURNER\", 30),\n",
        "      (7876, \"ADAMS\", 20),\n",
        "      (7900, \"JAMES\", 30),\n",
        "      (7934, \"MILLER\", 10),\n",
        "      (7902, \"FORD\", 20),\n",
        "      (7654, \"MARTIN\", 30))\n",
        "\n",
        "// Save sample data in the Parquet format\n",
        "import spark.implicits._\n",
        "val empData: DataFrame = employees.toDF(\"empId\", \"empName\", \"deptId\")\n",
        "val deptData: DataFrame = departments.toDF(\"deptId\", \"deptName\", \"location\")\n",
        "\n",
        "val empLocation: String = \"/<yourpath>/employees.parquet\"       //TODO ** customize this location path **\n",
        "val deptLocation: String = \"/<yourpath>/departments.parquet\"     //TODO ** customize this location path **\n",
        "empData.write.mode(\"overwrite\").parquet(empLocation)\n",
        "deptData.write.mode(\"overwrite\").parquet(deptLocation)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's verify the contents of parquet files we created above to make sure they contain expected records in correct format. We later use these data files to create Charm indexes and run sample queries.\n",
        "\n",
        "Running below cell, the output displays the rows in employee and department dataFrames in a tabular form. There should be 14 employees and 4 departments, each matching with one of triplets we created in previous cell."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "empDF: org.apache.spark.sql.DataFrame = [empId: int, empName: string ... 1 more field]\ndeptDF: org.apache.spark.sql.DataFrame = [deptId: int, deptName: string ... 1 more field]\n+-----+-------+------+\n|empId|empName|deptId|\n+-----+-------+------+\n| 7499|  ALLEN|    30|\n| 7521|   WARD|    30|\n| 7369|  SMITH|    20|\n| 7844| TURNER|    30|\n| 7876|  ADAMS|    20|\n| 7900|  JAMES|    30|\n| 7934| MILLER|    10|\n| 7839|   KING|    10|\n| 7566|  JONES|    20|\n| 7698|  BLAKE|    30|\n| 7782|  CLARK|    10|\n| 7788|  SCOTT|    20|\n| 7902|   FORD|    20|\n| 7654| MARTIN|    30|\n+-----+-------+------+\n\n+------+----------+--------+\n|deptId|  deptName|location|\n+------+----------+--------+\n|    10|Accounting|New York|\n|    40|Operations|  Boston|\n|    20|  Research|  Dallas|\n|    30|     Sales| Chicago|\n+------+----------+--------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// empLocation and deptLocation are the user defined locations above to save parquet files\n",
        "val empDF: DataFrame = spark.read.parquet(empLocation)\n",
        "val deptDF: DataFrame = spark.read.parquet(deptLocation)\n",
        "\n",
        "// Verify the data is available and correct\n",
        "empDF.show()\n",
        "deptDF.show()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Index Management\n",
        "Charm lets users create indexes on records scanned from persisted data files.\n",
        "Once successfully created, an entry correspondeing to the index is added to the Charm metadata.\n",
        "This metadata is later used during query processing to find and use proper indexes. If data changes, user can refresh an existing index to capture that. If an index is no longer required, user can drop it and force removing its data and metadata completely from Charm.\n",
        "Below sections show how such index management operations can be done in Charm.\n",
        "\n",
        "First, we need to import required libraries and create an instance of Charm. We later use this instance to invoke different Charm commands to create indexes on our sample data and modify those indexes.\n",
        "\n",
        "Output of running below cell shows a reference to the created instance of Charm."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "import com.microsoft.charm._\ncharm: com.microsoft.charm.Charm = com.microsoft.charm.Charm@1432f740"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Create an instance of Charm\n",
        "import com.microsoft.charm._\n",
        "\n",
        "val charm: Charm = Charm()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Indexes\n",
        "To create a Charm index, the user needs to provide 2 pieces of information:\n",
        "* DataFrame which shows the data to be indexed.\n",
        "* IndexConfig which specifies the *index name*, *indexed* and *included* columns of the index.\n",
        "\n",
        "We start by creating three Charm indexes on our sample data: two indexes on the department dataset named \"deptIndex1\" and \"deptIndex2\", and one index on the employee dataset named 'empIndex'. \n",
        "For each index, we need a corresponding IndexConfig to capture the name along with columns lists for the indexed and included columns. Running below cell creates these indexConfigs and its output lists them."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "import com.microsoft.charm.index.IndexConfig\nempIndexConfig: com.microsoft.charm.index.IndexConfig = [indexName: empIndex; indexedColumns: deptid; includedColumns: empname]\ndeptIndexConfig1: com.microsoft.charm.index.IndexConfig = [indexName: deptIndex1; indexedColumns: deptid; includedColumns: deptname]\ndeptIndexConfig2: com.microsoft.charm.index.IndexConfig = [indexName: deptIndex2; indexedColumns: location; includedColumns: deptname]"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Create index configurations\n",
        "import com.microsoft.charm.index.IndexConfig\n",
        "\n",
        "val empIndexConfig: IndexConfig = IndexConfig(\"empIndex\", Seq(\"deptId\"), Seq(\"empName\"))\n",
        "val deptIndexConfig1: IndexConfig = IndexConfig(\"deptIndex1\", Seq(\"deptId\"), Seq(\"deptName\"))\n",
        "val deptIndexConfig2: IndexConfig = IndexConfig(\"deptIndex2\", Seq(\"location\"), Seq(\"deptName\"))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we create three indexes using our index configurations. For this purpose, we invoke \"createIndex\" command on our Charm instance. This command requires an index configuration and the dataFrame containing rows to be indexed.\n",
        "Running below cell creates three indexes.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "import com.microsoft.charm.index.Index"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Create indexes from configurations\n",
        "import com.microsoft.charm.index.Index\n",
        "\n",
        "charm.createIndex(empDF, empIndexConfig)\n",
        "charm.createIndex(deptDF, deptIndexConfig1)\n",
        "charm.createIndex(deptDF, deptIndexConfig2)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List Indexes\n",
        "\n",
        "Below code shows how a user can list all available indexes in a Charm instance. It uses \"indexes\" command which returns information about existing indexes as a dataFrame. We can invoke valid operations on this dataFrame for checking its content or analyzing it further (for example filtering specific indexes or grouping them according to some desired property). \n",
        "\n",
        "Below cell uses dataFrame's 'show' action to fully print the rows and show details of our indexes in a tabular form. For each index, we can see all information Charm has stored about it in the metadata. \"config.indexName\", \"config.indexedColumns\", \"config.includedColumns\" and \"status.status\" are the fields that a user normally refers to. \"dfSignature\" is automatically generated by Charm and is unique for each index. \n",
        "Charm uses this signature internally to maintain the index and exploit it at query time. In the output,  all three indexes should have \"ACTIVE\" as status and their name, indexed columns, and included columns should match with what we defined in index configurations above.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+\n|config.indexName|config.indexedColumns|config.includedColumns|        schemaString|   signatureProvider|         dfSignature|      serializedPlan|numBuckets|             dirPath|status.value|stats.indexSize|\n+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+\n|      deptIndex1|             [deptId]|            [deptName]|`deptId` INT,`dep...|com.microsoft.cha...|0effc1610ae2e7c49...|Relation[deptId#3...|       200|abfss://datasets@...|      ACTIVE|              0|\n|      deptIndex2|           [location]|            [deptName]|`location` STRING...|com.microsoft.cha...|0effc1610ae2e7c49...|Relation[deptId#3...|       200|abfss://datasets@...|      ACTIVE|              0|\n|        empIndex|             [deptId]|             [empName]|`deptId` INT,`emp...|com.microsoft.cha...|30768c6c9b2533004...|Relation[empId#32...|       200|abfss://datasets@...|      ACTIVE|              0|\n+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "charm.indexes.show"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete Indexes\n",
        "A user can drop an existing index by using \"deleteIndex\" command and providing the index name. Index deletion does a soft delete: It mainly updates index's status in the Charm metadata from \"ACTIVE\" to \"DELETED\". This will exclude the dropped index from any future query optimization and Charm no longer picks that index for any query. However, index files for a deleted index still remain available so that the index could be restored if user asks for.\n",
        "\n",
        "Below cell deletes index with name \"deptIndex2\" and lists Charm metadata after that. The output should be similar to above cell for \"List Indexes\" except for \"deptIndex2\" which now should have its status changed into \"DELETED\"."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+\n|config.indexName|config.indexedColumns|config.includedColumns|        schemaString|   signatureProvider|         dfSignature|      serializedPlan|numBuckets|             dirPath|status.value|stats.indexSize|\n+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+\n|      deptIndex1|             [deptId]|            [deptName]|`deptId` INT,`dep...|com.microsoft.cha...|0effc1610ae2e7c49...|Relation[deptId#3...|       200|abfss://datasets@...|      ACTIVE|              0|\n|      deptIndex2|           [location]|            [deptName]|`location` STRING...|com.microsoft.cha...|0effc1610ae2e7c49...|Relation[deptId#3...|       200|abfss://datasets@...|     DELETED|              0|\n|        empIndex|             [deptId]|             [empName]|`deptId` INT,`emp...|com.microsoft.cha...|30768c6c9b2533004...|Relation[empId#32...|       200|abfss://datasets@...|      ACTIVE|              0|\n+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "charm.deleteIndex(\"deptIndex2\")\n",
        "\n",
        "charm.indexes.show"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Restore Indexes\n",
        "A user can use \"restoreIndex\" command to restore a deleted index. This will bring back the latest version of index into ACTIVE status and makes it usable again for queries. Below cell shows an example of \"restoreIndex\" usage. We delete \"deptIndex1\" and restore it. The output shows \"deptIndex1\" first went into the \"DELETED\" status after invoking \"deleteIndex\" command and came back to the \"ACTIVE\" status after calling \"restoreIndex\".\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+\n|config.indexName|config.indexedColumns|config.includedColumns|        schemaString|   signatureProvider|         dfSignature|      serializedPlan|numBuckets|             dirPath|status.value|stats.indexSize|\n+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+\n|      deptIndex1|             [deptId]|            [deptName]|`deptId` INT,`dep...|com.microsoft.cha...|0effc1610ae2e7c49...|Relation[deptId#3...|       200|abfss://datasets@...|     DELETED|              0|\n|      deptIndex2|           [location]|            [deptName]|`location` STRING...|com.microsoft.cha...|0effc1610ae2e7c49...|Relation[deptId#3...|       200|abfss://datasets@...|     DELETED|              0|\n|        empIndex|             [deptId]|             [empName]|`deptId` INT,`emp...|com.microsoft.cha...|30768c6c9b2533004...|Relation[empId#32...|       200|abfss://datasets@...|      ACTIVE|              0|\n+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+\n\n+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+\n|config.indexName|config.indexedColumns|config.includedColumns|        schemaString|   signatureProvider|         dfSignature|      serializedPlan|numBuckets|             dirPath|status.value|stats.indexSize|\n+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+\n|      deptIndex1|             [deptId]|            [deptName]|`deptId` INT,`dep...|com.microsoft.cha...|0effc1610ae2e7c49...|Relation[deptId#3...|       200|abfss://datasets@...|      ACTIVE|              0|\n|      deptIndex2|           [location]|            [deptName]|`location` STRING...|com.microsoft.cha...|0effc1610ae2e7c49...|Relation[deptId#3...|       200|abfss://datasets@...|     DELETED|              0|\n|        empIndex|             [deptId]|             [empName]|`deptId` INT,`emp...|com.microsoft.cha...|30768c6c9b2533004...|Relation[empId#32...|       200|abfss://datasets@...|      ACTIVE|              0|\n+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "charm.deleteIndex(\"deptIndex1\")\n",
        "\n",
        "charm.indexes.show\n",
        "\n",
        "charm.restoreIndex(\"deptIndex1\")\n",
        "\n",
        "charm.indexes.show"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vacuum Indexes\n",
        "The user can fully remove files and the metadata entry for a deleted index using \"vacuumIndex\" command. Once done, this action is irreversible as it physically deletes all the index files (i.e. it is a hard delete).\n",
        " \n",
        "Below cell vacuums \"deptIndex2\" and shows Charm metadata after vaccuming. You should see metadata entries for two indexes \"deptIndex1\" and \"empIndex\" both with \"ACTIVE\" status and no entry for \"deptIndex2\"."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+\n|config.indexName|config.indexedColumns|config.includedColumns|        schemaString|   signatureProvider|         dfSignature|      serializedPlan|numBuckets|             dirPath|status.value|stats.indexSize|\n+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+\n|      deptIndex1|             [deptId]|            [deptName]|`deptId` INT,`dep...|com.microsoft.cha...|0effc1610ae2e7c49...|Relation[deptId#3...|       200|abfss://datasets@...|      ACTIVE|              0|\n|        empIndex|             [deptId]|             [empName]|`deptId` INT,`emp...|com.microsoft.cha...|30768c6c9b2533004...|Relation[empId#32...|       200|abfss://datasets@...|      ACTIVE|              0|\n+----------------+---------------------+----------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+---------------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "charm.vacuumIndex(\"deptIndex2\")\n",
        "\n",
        "charm.indexes.show"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enable/Disable Charm\n",
        "\n",
        "Charm provides commands to enable or disable index usage with Spark.\n",
        "\n",
        "By using \"enableCharm\" command, Charm optimization rules become visible to the Spark optimizer and they will exploit existing Charm indexes to optimize user queries.\n",
        "\n",
        "By using \"disableCharm' command, Charm rules no longer apply during query optimization. You should note that disabling Charm has no impact on created indexes as they remain intact.\n",
        "\n",
        "Below cell shows how you can use these commands to enable or disable charm. The output simply shows a reference to the eixsting Spark session whose configuration is updated."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "res48: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@39fe1ddb\nres51: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@39fe1ddb"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Enable Charm\n",
        "spark.enableCharm\n",
        "\n",
        "// Disable Charm\n",
        "spark.disableCharm"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Index Usage\n",
        "In order to make Spark use Charm indexes during query processing, the user needs to make sure that Charm is enabled. Below cell enables Charm and creates two dataFrames containing our sample data records which we use for running example queries. For each dataFrame, a few sample rows are printed."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "res53: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@39fe1ddb\nempDFrame: org.apache.spark.sql.DataFrame = [empId: int, empName: string ... 1 more field]\ndeptDFrame: org.apache.spark.sql.DataFrame = [deptId: int, deptName: string ... 1 more field]\n+-----+-------+------+\n|empId|empName|deptId|\n+-----+-------+------+\n| 7499|  ALLEN|    30|\n| 7521|   WARD|    30|\n| 7369|  SMITH|    20|\n| 7844| TURNER|    30|\n| 7876|  ADAMS|    20|\n+-----+-------+------+\nonly showing top 5 rows\n\n+------+----------+--------+\n|deptId|  deptName|location|\n+------+----------+--------+\n|    10|Accounting|New York|\n|    40|Operations|  Boston|\n|    20|  Research|  Dallas|\n|    30|     Sales| Chicago|\n+------+----------+--------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Enable Charm\n",
        "spark.enableCharm\n",
        "\n",
        "val empDFrame: DataFrame = spark.read.parquet(empLocation)\n",
        "val deptDFrame: DataFrame = spark.read.parquet(deptLocation)\n",
        "\n",
        "empDFrame.show(5)\n",
        "deptDFrame.show(5)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Currently, Charm has rules to exploit indexes for two groups of queries: \n",
        "* Selection queries with lookup or range selection filtering predicates.\n",
        "* Join queries with an equality join predicate (i.e. Equi-joins).\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First example query does a lookup on department records (see below cell).\n",
        "\n",
        "Output of running below cell shows: \n",
        "- query result, which is a single department name.\n",
        "- query plan, Spark used to run the query. The \"FileScan\" operator at the bottom of the plan shows the datasource where the records were read from. The location of this file indicates the path to the latest version of the \"deptIndex1\" index. This shows  that according to the query and using Charm optimization rules, Spark decided to exploit the proper index at runtime.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "eqFilter: org.apache.spark.sql.DataFrame = [deptName: string]\n+--------+\n|deptName|\n+--------+\n|Research|\n+--------+\n\n== Parsed Logical Plan ==\n'Project [unresolvedalias('deptName, None)]\n+- Filter (deptId#533 = 20)\n   +- Relation[deptId#533,deptName#534,location#535] parquet\n\n== Analyzed Logical Plan ==\ndeptName: string\nProject [deptName#534]\n+- Filter (deptId#533 = 20)\n   +- Relation[deptId#533,deptName#534,location#535] parquet\n\n== Optimized Logical Plan ==\nProject [deptName#534]\n+- Filter (isnotnull(deptId#533) && (deptId#533 = 20))\n   +- Relation[deptId#533,deptName#534,location#535] parquet\n\n== Physical Plan ==\n*(1) Project [deptName#534]\n+- *(1) Filter (isnotnull(deptId#533) && (deptId#533 = 20))\n   +- *(1) FileScan parquet [deptId#533,deptName#534] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://datasets@charmbenchmark.dfs.core.windows.net/synapse/workspaces/charmon..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId), EqualTo(deptId,20)], ReadSchema: struct<deptId:int,deptName:string>"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Filter with equality predicate\n",
        "\n",
        "// The following query picks the department whose deptId is 20.\n",
        "// This query is equivalent to:\n",
        "// \"SELECT deptName FROM departments\n",
        "//  WHERE deptId = 20\"\n",
        "\n",
        "val eqFilter: DataFrame = deptDFrame.filter(\"deptId = 20\").select(\"deptName\")\n",
        "eqFilter.show()\n",
        "\n",
        "eqFilter.explain(true)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second example is a range selection query on department records. Similar to the previous example, output of below cell shows the query results (names of two departments) and the query plan. The location of data file in the FileScan operator shows that 'deptIndex1\" was used to run the query.   \n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "rangeFilter: org.apache.spark.sql.DataFrame = [deptName: string]\n+----------+\n|  deptName|\n+----------+\n|Operations|\n|     Sales|\n+----------+\n\n== Parsed Logical Plan ==\n'Project [unresolvedalias('deptName, None)]\n+- Filter (deptId#533 > 20)\n   +- Relation[deptId#533,deptName#534,location#535] parquet\n\n== Analyzed Logical Plan ==\ndeptName: string\nProject [deptName#534]\n+- Filter (deptId#533 > 20)\n   +- Relation[deptId#533,deptName#534,location#535] parquet\n\n== Optimized Logical Plan ==\nProject [deptName#534]\n+- Filter (isnotnull(deptId#533) && (deptId#533 > 20))\n   +- Relation[deptId#533,deptName#534,location#535] parquet\n\n== Physical Plan ==\n*(1) Project [deptName#534]\n+- *(1) Filter (isnotnull(deptId#533) && (deptId#533 > 20))\n   +- *(1) FileScan parquet [deptId#533,deptName#534] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://datasets@charmbenchmark.dfs.core.windows.net/synapse/workspaces/charmon..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId), GreaterThan(deptId,20)], ReadSchema: struct<deptId:int,deptName:string>"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Filter with range selection predicate\n",
        "\n",
        "// The following query picks all departments with deptId greater than 20.\n",
        "// This query is equivalent to:\n",
        "// \"SELECT deptName FROM departments\n",
        "//  WHERE deptId > 20\"\n",
        "\n",
        "val rangeFilter: DataFrame = deptDFrame.filter(\"deptId > 20\").select(\"deptName\")\n",
        "rangeFilter.show()\n",
        "\n",
        "rangeFilter.explain(true)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our third example is query joining department and employee records on the department id.\n",
        "\n",
        "Output of running below cell shows the query results which are the names of 14 employees and the name of department each employee works in. The query plan is also included in the output. Checking the file locations for two FileScan operators shows that Spark used \"empIndex\" and \"deptIndex1\" indexes to run the query.   \n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "eqJoin: org.apache.spark.sql.DataFrame = [empName: string, deptName: string]\n+-------+----------+\n|empName|  deptName|\n+-------+----------+\n|  SMITH|  Research|\n|  JONES|  Research|\n|   FORD|  Research|\n|  ADAMS|  Research|\n|  SCOTT|  Research|\n|   KING|Accounting|\n|  CLARK|Accounting|\n| MILLER|Accounting|\n|  JAMES|     Sales|\n|  BLAKE|     Sales|\n| MARTIN|     Sales|\n|  ALLEN|     Sales|\n|   WARD|     Sales|\n| TURNER|     Sales|\n+-------+----------+\n\n== Parsed Logical Plan ==\nProject [empName#528, deptName#534]\n+- Join Inner, (deptId#529 = deptId#533)\n   :- Relation[empId#527,empName#528,deptId#529] parquet\n   +- Relation[deptId#533,deptName#534,location#535] parquet\n\n== Analyzed Logical Plan ==\nempName: string, deptName: string\nProject [empName#528, deptName#534]\n+- Join Inner, (deptId#529 = deptId#533)\n   :- Relation[empId#527,empName#528,deptId#529] parquet\n   +- Relation[deptId#533,deptName#534,location#535] parquet\n\n== Optimized Logical Plan ==\nProject [empName#528, deptName#534]\n+- Join Inner, (deptId#529 = deptId#533)\n   :- Project [empName#528, deptId#529]\n   :  +- Filter isnotnull(deptId#529)\n   :     +- Relation[empName#528,deptId#529] parquet\n   +- Project [deptId#533, deptName#534]\n      +- Filter isnotnull(deptId#533)\n         +- Relation[deptId#533,deptName#534] parquet\n\n== Physical Plan ==\n*(3) Project [empName#528, deptName#534]\n+- *(3) SortMergeJoin [deptId#529], [deptId#533], Inner\n   :- *(1) Project [empName#528, deptId#529]\n   :  +- *(1) Filter isnotnull(deptId#529)\n   :     +- *(1) FileScan parquet [deptId#529,empName#528] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://datasets@charmbenchmark.dfs.core.windows.net/synapse/workspaces/charmon..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,empName:string>, SelectedBucketsCount: 200 out of 200\n   +- *(2) Project [deptId#533, deptName#534]\n      +- *(2) Filter isnotnull(deptId#533)\n         +- *(2) FileScan parquet [deptId#533,deptName#534] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://datasets@charmbenchmark.dfs.core.windows.net/synapse/workspaces/charmon..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,deptName:string>, SelectedBucketsCount: 200 out of 200"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Join\n",
        "\n",
        "// The following query joins employees with departments on deptId. \n",
        "// This query is equivalent to:\n",
        "// \"SELECT employees.deptId, empName, departments.deptId, deptName\n",
        "//  FROM   employees, departments \n",
        "//  WHERE  employees.deptId = departments.deptId\"\n",
        "\n",
        "val eqJoin: DataFrame =\n",
        "      empDFrame.\n",
        "      join(deptDFrame, empDFrame(\"deptId\") === deptDFrame(\"deptId\")).\n",
        "      select(empDFrame(\"empName\"), deptDFrame(\"deptName\"))\n",
        "\n",
        "eqJoin.show()\n",
        "\n",
        "eqJoin.explain(true)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Support for SQL Semantics\n",
        "\n",
        "The index usage is transparent to whether the user uses dataframe api or spark sql api. The following example shows the same join example as before, in sql form, showing the use of indexes if applicable."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "joinQuery: org.apache.spark.sql.DataFrame = [empName: string, deptName: string]\n+-------+----------+\n|empName|  deptName|\n+-------+----------+\n|  SMITH|  Research|\n|  JONES|  Research|\n|   FORD|  Research|\n|  ADAMS|  Research|\n|  SCOTT|  Research|\n|   KING|Accounting|\n|  CLARK|Accounting|\n| MILLER|Accounting|\n|  JAMES|     Sales|\n|  BLAKE|     Sales|\n| MARTIN|     Sales|\n|  ALLEN|     Sales|\n|   WARD|     Sales|\n| TURNER|     Sales|\n+-------+----------+\n\n== Parsed Logical Plan ==\n'Project ['EMP.empName, 'DEPT.deptName]\n+- 'Filter ('EMP.deptId = 'DEPT.deptId)\n   +- 'Join Inner\n      :- 'UnresolvedRelation `EMP`\n      +- 'UnresolvedRelation `DEPT`\n\n== Analyzed Logical Plan ==\nempName: string, deptName: string\nProject [empName#528, deptName#534]\n+- Filter (deptId#529 = deptId#533)\n   +- Join Inner\n      :- SubqueryAlias `emp`\n      :  +- Relation[empId#527,empName#528,deptId#529] parquet\n      +- SubqueryAlias `dept`\n         +- Relation[deptId#533,deptName#534,location#535] parquet\n\n== Optimized Logical Plan ==\nProject [empName#528, deptName#534]\n+- Join Inner, (deptId#529 = deptId#533)\n   :- Project [empName#528, deptId#529]\n   :  +- Filter isnotnull(deptId#529)\n   :     +- Relation[empId#527,empName#528,deptId#529] parquet\n   +- Project [deptId#533, deptName#534]\n      +- Filter isnotnull(deptId#533)\n         +- Relation[deptId#533,deptName#534,location#535] parquet\n\n== Physical Plan ==\n*(5) Project [empName#528, deptName#534]\n+- *(5) SortMergeJoin [deptId#529], [deptId#533], Inner\n   :- *(2) Sort [deptId#529 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(deptId#529, 200)\n   :     +- *(1) Project [empName#528, deptId#529]\n   :        +- *(1) Filter isnotnull(deptId#529)\n   :           +- *(1) FileScan parquet [deptId#529,empName#528] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://datasets@charmbenchmark.dfs.core.windows.net/synapse/workspaces/charmon..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,empName:string>\n   +- *(4) Sort [deptId#533 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(deptId#533, 200)\n         +- *(3) Project [deptId#533, deptName#534]\n            +- *(3) Filter isnotnull(deptId#533)\n               +- *(3) FileScan parquet [deptId#533,deptName#534] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://datasets@charmbenchmark.dfs.core.windows.net/apdave/departments.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,deptName:string>"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "empDFrame.createOrReplaceTempView(\"EMP\")\n",
        "deptDFrame.createOrReplaceTempView(\"DEPT\")\n",
        "\n",
        "val joinQuery = spark.sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\")\n",
        "\n",
        "spark.conf.set(\"spark.charm.explain.displayMode\", \"html\")\n",
        "\n",
        "joinQuery.show()\n",
        "joinQuery.explain(true)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explain Api\n",
        "Charm allows users to compare their original plan vs the updated index-dependent plan before running their query. They have an option to choose from html/plaintext/console mode to display the command output. Here's an example with HTML. The highlighted section represents the difference between original and updated plans."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/html": "<pre>=============================================================<br>Plan with indexes:<br>=============================================================<br>Project [empName#528, deptName#534]<br>+- SortMergeJoin [deptId#529], [deptId#533], Inner<br>   <b style=\"background:LightGreen\">:- *(1) Project [empName#528, deptId#529]</b><br>   <b style=\"background:LightGreen\">:  +- *(1) Filter isnotnull(deptId#529)</b><br>   <b style=\"background:LightGreen\">:     +- *(1) FileScan parquet [deptId#529,empName#528] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://datasets@charmbenchmark.dfs.core.windows.net/synapse/workspaces/charmon..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,empName:string>, SelectedBucketsCount: 200 out of 200</b><br>   <b style=\"background:LightGreen\">+- *(2) Project [deptId#533, deptName#534]</b><br>      <b style=\"background:LightGreen\">+- *(2) Filter isnotnull(deptId#533)</b><br>         <b style=\"background:LightGreen\">+- *(2) FileScan parquet [deptId#533,deptName#534] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://datasets@charmbenchmark.dfs.core.windows.net/synapse/workspaces/charmon..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,deptName:string>, SelectedBucketsCount: 200 out of 200</b><br><br>=============================================================<br>Plan without indexes:<br>=============================================================<br>Project [empName#528, deptName#534]<br>+- SortMergeJoin [deptId#529], [deptId#533], Inner<br>   <b style=\"background:LightGreen\">:- *(2) Sort [deptId#529 ASC NULLS FIRST], false, 0</b><br>   <b style=\"background:LightGreen\">:  +- Exchange hashpartitioning(deptId#529, 200)</b><br>   <b style=\"background:LightGreen\">:     +- *(1) Project [empName#528, deptId#529]</b><br>   <b style=\"background:LightGreen\">:        +- *(1) Filter isnotnull(deptId#529)</b><br>   <b style=\"background:LightGreen\">:           +- *(1) FileScan parquet [empName#528,deptId#529] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://datasets@charmbenchmark.dfs.core.windows.net/apdave/employees.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<empName:string,deptId:int></b><br>   <b style=\"background:LightGreen\">+- *(4) Sort [deptId#533 ASC NULLS FIRST], false, 0</b><br>      <b style=\"background:LightGreen\">+- Exchange hashpartitioning(deptId#533, 200)</b><br>         <b style=\"background:LightGreen\">+- *(3) Project [deptId#533, deptName#534]</b><br>            <b style=\"background:LightGreen\">+- *(3) Filter isnotnull(deptId#533)</b><br>               <b style=\"background:LightGreen\">+- *(3) FileScan parquet [deptId#533,deptName#534] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://datasets@charmbenchmark.dfs.core.windows.net/apdave/departments.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,deptName:string></b><br><br>=============================================================<br>Indexes used:<br>=============================================================<br>deptIndex1:abfss://datasets@charmbenchmark.dfs.core.windows.net/synapse/workspaces/charmonbbc/indexes/public/deptIndex1/v__=0<br>empIndex:abfss://datasets@charmbenchmark.dfs.core.windows.net/synapse/workspaces/charmonbbc/indexes/public/empIndex/v__=0<br><br></pre>"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "spark.conf.set(\"spark.charm.explain.displayMode\", \"html\")\n",
        "charm.explain(eqJoin) { displayHTML }"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Refresh Indexes\n",
        "If the original data on which an index was created changes, then the index will no longer capture the latest state of data. The user can refresh such a stale index using \"refreshIndex\" command. This causes the index to be fully rebuilt and updates it accroding to the latest data records.\n",
        "Below two cells show an example for this scenario:\n",
        "- First cell adds two more departments to the original departments data. It reads and prints list of departments to verify new departments are added correctly. The output shows 6 departments in total: four old ones and two new. Invoking \"refreshIndex\" updates \"deptIndex1\" so index captures new departments.\n",
        "- Second cell runs our range selection query example. The results should now contain four departments: two are the ones, seen before when we ran the query above, and two are the new departments we just added."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "extraDepartments: Seq[(Int, String, String)] = List((50,Inovation,Seattle), (60,Human Resources,San Francisco))\nextraDeptData: org.apache.spark.sql.DataFrame = [deptId: int, deptName: string ... 1 more field]\ndeptDFrameUpdated: org.apache.spark.sql.DataFrame = [deptId: int, deptName: string ... 1 more field]\n+------+---------------+-------------+\n|deptId|       deptName|     location|\n+------+---------------+-------------+\n|    60|Human Resources|San Francisco|\n|    10|     Accounting|     New York|\n|    50|      Inovation|      Seattle|\n|    40|     Operations|       Boston|\n|    20|       Research|       Dallas|\n|    30|          Sales|      Chicago|\n+------+---------------+-------------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "val extraDepartments = Seq(\n",
        "      (50, \"Inovation\", \"Seattle\"),\n",
        "\t  (60, \"Human Resources\", \"San Francisco\"))\n",
        "\t  \n",
        "val extraDeptData: DataFrame = extraDepartments.toDF(\"deptId\", \"deptName\", \"location\")\n",
        "extraDeptData.write.mode(\"Append\").parquet(deptLocation)\n",
        "\n",
        "val deptDFrameUpdated: DataFrame = spark.read.parquet(deptLocation)\n",
        "\n",
        "deptDFrameUpdated.show(10)\n",
        "\n",
        "charm.refreshIndex(\"deptIndex1\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "newRangeFilter: org.apache.spark.sql.DataFrame = [deptName: string]\n+---------------+\n|       deptName|\n+---------------+\n|Human Resources|\n|      Inovation|\n|     Operations|\n|          Sales|\n+---------------+\n\n== Parsed Logical Plan ==\n'Project [unresolvedalias('deptName, None)]\n+- Filter (deptId#674 > 20)\n   +- Relation[deptId#674,deptName#675,location#676] parquet\n\n== Analyzed Logical Plan ==\ndeptName: string\nProject [deptName#675]\n+- Filter (deptId#674 > 20)\n   +- Relation[deptId#674,deptName#675,location#676] parquet\n\n== Optimized Logical Plan ==\nProject [deptName#675]\n+- Filter (isnotnull(deptId#674) && (deptId#674 > 20))\n   +- Relation[deptId#674,deptName#675,location#676] parquet\n\n== Physical Plan ==\n*(1) Project [deptName#675]\n+- *(1) Filter (isnotnull(deptId#674) && (deptId#674 > 20))\n   +- *(1) FileScan parquet [deptId#674,deptName#675] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://datasets@charmbenchmark.dfs.core.windows.net/synapse/workspaces/charmon..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId), GreaterThan(deptId,20)], ReadSchema: struct<deptId:int,deptName:string>"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "val newRangeFilter: DataFrame = deptDFrameUpdated.filter(\"deptId > 20\").select(\"deptName\")\n",
        "newRangeFilter.show()\n",
        "\n",
        "newRangeFilter.explain(true)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query speed-up using Charm\n",
        "Depending on datasize, specific queries and index configs, Charm provides various degree of acceleration. Here is an example of running TPCH query 6 (Q6) over 100GB TPCH dataset. For simplicity, an index for Q6 is already created and ready to be exploited. \n",
        "\n",
        "By running Q6 in Charm enabled vs disabled mode, we usually see a query time difference between 0.5-1 min vs. 18-30 mins.\n",
        "\n",
        "Here are the steps to setup and run TPCH Q6:\n",
        "- Specify the index location (where the created index is stored) in the spark config.\n",
        "- Prepare data and define Q6.\n",
        "- Run Q6 in Charm-enabled and Charm-disabled mode, and see the query time difference."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "abfss://defaultdata@charmdatabugbash.dfs.core.windows.net/tpch-parquet-100GB-bbash-4bucket-indexes"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Set charm system path, or index location, in spark config.\n",
        "spark.conf.set(\"spark.charm.system.path\", \"abfss://defaultdata@charmdatabugbash.dfs.core.windows.net/tpch-parquet-100GB-bbash-4bucket-indexes\")\n",
        "println(spark.conf.get(\"spark.charm.system.path\"))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "lineitemPath: String = abfss://defaultdata@charmdatabugbash.dfs.core.windows.net/tpch-parquet-100GB/lineitem\nlineitem: org.apache.spark.sql.DataFrame = [l_orderkey: int, l_partkey: int ... 14 more fields]\nrunQ6: ()Unit"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Table preparation.\n",
        "val lineitemPath = \"abfss://defaultdata@charmdatabugbash.dfs.core.windows.net/tpch-parquet-100GB/lineitem\"\n",
        "val lineitem = spark.read.parquet(lineitemPath)\n",
        "\n",
        "// Define Q6.\n",
        "def runQ6(): Unit = {\n",
        "    val q6_start = System.currentTimeMillis()\n",
        "    lineitem.filter($\"l_shipdate\" >= \"1994-01-01\" && $\"l_shipdate\" < \"1995-01-01\" && $\"l_discount\" >= 0.05 && $\"l_discount\" <= 0.07 && $\"l_quantity\" < 24).agg(sum($\"l_extendedprice\" * $\"l_discount\")).show()\n",
        "    val q6_end = System.currentTimeMillis()\n",
        "    println(\"Q6 execution time: \" + (q6_end - q6_start)/1000.00 + \" sec.\")\n",
        "}"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "res127: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6c8a1072\n==== Enable charm ====\n+-----------------------------------+\n|sum((l_extendedprice * l_discount))|\n+-----------------------------------+\n|               1.233042688846342...|\n+-----------------------------------+\n\nQ6 execution time: 38.183 sec."
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Run Q6 in Charm-enabled mode.\n",
        "spark.enableCharm\n",
        "println(\"==== Enable charm ====\")\n",
        "runQ6()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "res133: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6c8a1072\n==== Disable charm ====\n+-----------------------------------+\n|sum((l_extendedprice * l_discount))|\n+-----------------------------------+\n|               1.233042688846365...|\n+-----------------------------------+\n\nQ6 execution time: 1104.75 sec."
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Run Q6 in Charm-disabled mode.\n",
        "spark.disableCharm\n",
        "println(\"==== Disable charm ====\")\n",
        "runQ6()"
      ],
      "attachments": {}
    }
  ]
}